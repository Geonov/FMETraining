Let’s continue to work on the workspace that processes a dataset of cell phone signals.

We’ve already deconstructed the log and cleaned up the Readers and Writers. Now let’s use our new knowledge of transformation performance to try and speed up the workspace.

**1)** Start Workbench

Open the workspace C:\FMEData2015\Workspaces\DesktopAdvanced\Exercise2c-Begin.fmw which follows on from exercise 2b.

**2)** Check for Extra Transformers

The first aspect of the workspace to check is any extra transformers that aren’t needed and that will be slowing performance. The most obvious is the Logger transformer. It was presumably used for debugging the original workspace but is now doing nothing for us.

So, delete the Logger transformer attached to the CSV Reader.

**3)** Remove Attributes

Another quick fix we can do is to remove any attributes we don’t need, right at the start of the workspace. Check the schemas of the Reader and Writer feature types:

The Readers contain quite a lot of attributes, on both datasets. The Writers contain very few attributes, and the GoodLocations feature type has none at all. This suggests we can remove some attributes that are not going to be needed in the output.

Put an AttributeKeeper transformer after the Neighborhood feature type, but before the Clipper.

Use it to keep only the NeighborhoodName attribute.

Now do the same to the CSV (signal) data, keeping only the attributes StationID, Power, and Quality.

<table style="border-spacing: 0px">
<tr>
<td style="vertical-align:middle;background-color:darkorange;border: 2px solid darkorange">
<i class="fa fa-quote-left fa-lg fa-pull-left fa-fw" style="color:white;padding-right: 12px;vertical-align:text-top"></i>
<span style="color:white;font-size:x-large;font-weight: bold;font-family:serif">Jake Speedie says…</span>
</td>
</tr>

<tr>
<td style="border: 1px solid darkorange">
<span style="font-family:serif; font-style:italic; font-size:larger">
“Do we really need to remove attributes from only six neighborhood
features?
Yes! Because we’re copying them onto (depending on the source dataset used) 1.7
million CSV features.”
</span>
</td>
</tr>
</table>

**4)** Check Group-Based Processes

The Clipper is a group-based transformer; it has to be since it is processing both the neighborhoods and the cell signal data.

There’s no real indication this is the wrong transformer to use (although there are others) but we should check if there’s a way to turn the transformer into one that operates on a feature basis.

Open the Clipper parameters. Notice that there is a parameter for Clipper Type. Change this to Clippers First:

This will make this a feature-based transformer. Each clippee will not need to be cached because the full set of clippers is already known.

However, we have to make sure the Clippers really do arrive first, and this we can do by making the Clippers the first Reader in the Navigator window.

So, right-click the VancouverNeighborhoods Reader in the Navigator window and choose Move Up to bring it to the top of the list:

**5)** Run Workspace

Let’s run the workspace to see what we have so far.

Remember, after Reader improvements we had this result:

*FME Session Duration: 2 minutes 26.5 seconds. (CPU: 133.8s user, 12.4s system)*

*END - ProcessID: 98972, peak process memory usage: 1734748 kB*

The result now is:

*FME Session Duration: 2 minutes 16.6 seconds. (CPU: 126.6s user, 6.9s system)*

*END - ProcessID: 102032, peak process memory usage: 109780 kB*

It’s improving (especially the memory use). But I think we can still do better!

**6)** Rearrange Transformers

Looking at the workspace, the Neighborhood attribute is only required by the bad (low power) features. It isn’t needed by the good locations.

But, we’re still attaching the information onto all of the features, good or bad.

We could prevent that by moving the Tester transformer to before the Clipper.

So, select the Tester transformer and press Ctrl+X to cut it from the workspace. Notice that the connections are healed automatically, though they aren’t quite right.

Now press Ctrl+V to paste the Tester back into the workspace, but unconnected. Now drag it into the CSV data stream, but before it reaches the Clipper:

Finally, let’s fix the feature mapping.

Click on the connection from Clipper:Inside > GoodLocations, making sure to click closer to the Clipper than the feature type.

Then drag that connection onto the Tester:Failed port.

Re-run the workspace. The result will be something like this:

*FME Session Duration: 1 minutes 41.6 seconds. (CPU: 88.7s user, 12.2s system)*

*END - ProcessID: 220504, peak process memory usage: 110028 kB*

Hurrah! Compared to the original log we’re over 50% faster with 95% less memory use!

<table style="border-spacing: 0px">
<tr>
<td style="vertical-align:middle;background-color:darkorange;border: 2px solid darkorange">
<i class="fa fa-cogs fa-lg fa-pull-left fa-fw" style="color:white;padding-right: 12px;vertical-align:text-top"></i>
<span style="color:white;font-size:x-large;font-weight: bold;font-family:serif">Advanced Exercise</span>
</td>
</tr>

<tr>
<td style="border: 1px solid darkorange">
<span style="font-family:serif; font-style:italic; font-size:larger">
There is, perhaps, one other way we can upgrade this workspace’s performance. It’s
radical and unintuitive, but it might just work!
</span>
</td>
</tr>
</table>

Do you remember the previous tip about using the PointCloud XYZ reader for CSV data? Let’s give that a try and see what we can do.

Open the workspace C:\FMEData\Workspaces\DesktopAdvanced\Exercise2c-Begin-Advanced.fmw

Firstly, delete the CSV Reader (and feature types), the AttributeRemover/Keeper for the CSV data, and the Tester transformer too.

Now select Reader > Add New Reader and in the Add Reader dialog enter the following values:

Reader Format Point Cloud XYZ

Reader Dataset C:\FMEData2015\Data\CellSignals\CellSignals2015.csv

What are going to be key here are the Reader parameters, so click the Parameters button.

Firstly make sure the Separator Character is set to a comma (not “space”). Also check the option for "File Has Field Names."

Then in the Point Cloud Component Map, set the following:

You can type a component name, which is what you will have to do for all of them except the X and Y fields.

This will cause each column in the CSV data to be stored as a point cloud component.

It’s important to get this exactly right, as you will have to re-add the Reader to fix any problems.

Click OK to close the dialog and OK again to add the Reader.

A point cloud feature is not the same as a number of point features, so we will have to convert the data at some stage. However, let’s see if we can make use of some point cloud functionality first.

Add a PointCloudFilter transformer.
This is the point cloud equivalent to a Tester and may be quicker than testing each point individually.

Open the parameters dialog for this transformer.

Under Expression select the option to open an arithmetic editor. In the editor, look under the list of Point Cloud components to the left and double-click on <user component> at the bottom of that list. Enter Power as the component name.

Now click on the end of the expression, enter a less than operator (<) and type the value -125.
The dialog should look like so:

Click OK to close that dialog. Enter a new port name of Bad Signal. Click OK.

The workspace now looks like this, with a new output port:

Add two PointCloudCoercer transformers to the workspace, one attached to each output from the PointCloudFilter. One output should be directed to the Clipper:Clippee port, the other to the GoodLocations output feature type:

The final step is to set the coercer parameters and expose some attributes.

Open the parameters dialogs for each of the PointCloudCoercers in turn.

In both cases, set Output Geometry to Individual Points.

Again, in both cases, set Preserve Point Components As to Attributes.

For the <Unfiltered> data, which doesn’t need attributes, leave the Point Components to Preserve section empty. For the Bad Signal data, enter the following component names to extract them as attributes:
- StationID
- Power
- Quality

Now save and run the workspace. This time the log will report the performance as:

*FME Session Duration: 54.7 seconds. (CPU: 43.1s user, 10.9s system)*

*END - ProcessID: 111126, peak process memory usage: 124744 kB*

Well done! The memory use is up slightly, but the workspace is running faster than ever.